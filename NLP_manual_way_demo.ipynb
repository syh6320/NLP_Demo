{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation, digits\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(corpus, pattern = None):\n",
    "    '''\n",
    "        input: \n",
    "            corpus, a list of documents\n",
    "            pattern, if not None, we use RegexpTokenizer\n",
    "                     else we use word_tokenize\n",
    "        remove lower, stop words, punctuations, \n",
    "    '''\n",
    "    tokens = None\n",
    "    stop_words = stopwords.words('english')\n",
    "    if pattern is not None:\n",
    "        tokenizer = RegexpTokenizer.tokenize(pattern)\n",
    "        tokens_list = [tokenizer.tokenize(doc.lower()) for doc in corpus]\n",
    "    else:\n",
    "        tokens_list = [word_tokenize(doc.lower().translate(None, punctuation + digits)) for doc in corpus]\n",
    "    \n",
    "    return [[token for token in tokens if token not in stop_words] for tokens in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmize(tokens, stemmizer):\n",
    "    return [stemmizer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmize(tokens, lemmizer):\n",
    "    return [lemmizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_or_lem(tokens_list, stem = True):\n",
    "    '''\n",
    "        input: \n",
    "            tokens_list:\n",
    "            stem, default is True. If False we use lemmize\n",
    "        output:\n",
    "            stemmized or lemmatized tokens_list\n",
    "    '''\n",
    "    if stem:\n",
    "        stemmizer = PorterStemmer()\n",
    "        return [stemmize(tokens, stemmizer) for tokens in tokens_list]\n",
    "    else:\n",
    "        lemmizer = WordNetLemmatizer()\n",
    "        return [lemmize(tokens, lemmizer) for tokens in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(sl_tokens_list):\n",
    "    '''\n",
    "        get sorted vocabularies\n",
    "    '''\n",
    "    vocabulary = [token for tokens in sl_tokens_list for token in tokens]\n",
    "    return sorted(list(set(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_vector(tokens, vocabulary):\n",
    "    '''\n",
    "        input:\n",
    "            tokens are a list of tokens\n",
    "        output:\n",
    "            bag of word for this tokens\n",
    "    '''\n",
    "    bag_of_words = Counter(tokens)\n",
    "    token_vector = np.zeros(len(vocabulary))\n",
    "    for word_index, word in enumerate(vocabulary):\n",
    "        if word in bag_of_words:\n",
    "            token_vector[word_index] += bag_of_words[word]\n",
    "    return token_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bag_of_word(sl_tokens_list, vocabulary):\n",
    "    '''\n",
    "        get bag of word for all tokens_list\n",
    "    '''\n",
    "    return np.array([bow_vector(tokens, vocabulary) for tokens in sl_tokens_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf_matrix(bow):\n",
    "    '''\n",
    "        input:\n",
    "            bow is bag of word\n",
    "        output:\n",
    "            normalized tf matrix\n",
    "    '''\n",
    "    tf_matrix = deepcopy(bow)\n",
    "    for i in xrange(len(bog)):\n",
    "        tf_matrix[i] = tf_matrix[i] / np.sum(tf_matrix[i])\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(bow, vocabulary):\n",
    "    N = len(bow)\n",
    "    token_fre_array = np.sum(bow, axis = 0) + 1\n",
    "    return np.log(N / token_fre_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf_idf_matrix(bow, vocabulary):\n",
    "    tf_matrix = get_tf_matrix(bow)\n",
    "    idf = get_idf(bow,vocabulary)\n",
    "    return tf_matrix * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(corpus):\n",
    "    tokens_list = get_tokens(corpus)\n",
    "    sl_tokens_list = stem_or_lem(tokens_list, stem = False)\n",
    "    vocabulary = get_vocabulary(sl_tokens_list)\n",
    "    bow = get_bag_of_word(sl_tokens_list, vocabulary)\n",
    "    print bow\n",
    "    tf_idf_matrix = get_tf_idf_matrix(bog,vocabulary)\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = 'Wise people thinking they shop  are foolish!'\n",
    "doc2 = 'Foolish think shopping foolish people 4 * ( think they are wise wise'\n",
    "doc3 = 'I am definitely wise; so this irritates me'\n",
    "doc4 = '-- Trump is for sure like definitely foolish'\n",
    "corpus = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.]\n",
      " [ 0.  2.  0.  0.  1.  0.  1.  0.  2.  0.  0.  2.]\n",
      " [ 1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = main(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.04462871,  0.        ,  0.        ,  0.05753641,\n",
       "         0.13862944,  0.        ,  0.        ,  0.        ,  0.13862944,\n",
       "         0.        , -0.04462871],\n",
       "       [ 0.        , -0.05578589,  0.        ,  0.        ,  0.03596026,\n",
       "         0.        ,  0.0866434 ,  0.        ,  0.07192052,  0.        ,\n",
       "         0.        , -0.05578589],\n",
       "       [ 0.09589402, -0.        ,  0.23104906,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.07438118],\n",
       "       [ 0.05753641, -0.04462871,  0.        ,  0.13862944,  0.        ,\n",
       "         0.        ,  0.        ,  0.13862944,  0.        ,  0.        ,\n",
       "         0.13862944, -0.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
